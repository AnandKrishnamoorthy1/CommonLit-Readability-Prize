{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "humanitarian-struggle",
   "metadata": {
    "papermill": {
     "duration": 0.040279,
     "end_time": "2021-08-02T19:59:06.707261",
     "exception": false,
     "start_time": "2021-08-02T19:59:06.666982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook multiple models:\n",
    "\n",
    "https://www.kaggle.com/crained/clr-roberta-two?scriptVersionId=67782065: Score - 0.463\n",
    "https://www.kaggle.com/crained/fork-of-fork-of-yum-yum-yum-93f968?scriptVersionId=62716960: Score - 0.480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-empire",
   "metadata": {
    "papermill": {
     "duration": 0.054012,
     "end_time": "2021-08-02T19:59:06.799117",
     "exception": false,
     "start_time": "2021-08-02T19:59:06.745105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sacred-parish",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:06.905691Z",
     "iopub.status.busy": "2021-08-02T19:59:06.903901Z",
     "iopub.status.idle": "2021-08-02T19:59:09.944440Z",
     "shell.execute_reply": "2021-08-02T19:59:09.943838Z",
     "shell.execute_reply.started": "2021-08-02T17:27:01.176397Z"
    },
    "papermill": {
     "duration": 3.085524,
     "end_time": "2021-08-02T19:59:09.944633",
     "exception": false,
     "start_time": "2021-08-02T19:59:06.859109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pregnant-croatia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.067318Z",
     "iopub.status.busy": "2021-08-02T19:59:10.066585Z",
     "iopub.status.idle": "2021-08-02T19:59:10.069817Z",
     "shell.execute_reply": "2021-08-02T19:59:10.070261Z",
     "shell.execute_reply.started": "2021-08-02T17:27:04.010759Z"
    },
    "papermill": {
     "duration": 0.087729,
     "end_time": "2021-08-02T19:59:10.070428",
     "exception": false,
     "start_time": "2021-08-02T19:59:09.982699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 256\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "ROBERTA_PATH = \"/kaggle/input/roberta-base\"\n",
    "TOKENIZER_PATH = \"/kaggle/input/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entire-amber",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.148542Z",
     "iopub.status.busy": "2021-08-02T19:59:10.147953Z",
     "iopub.status.idle": "2021-08-02T19:59:10.164561Z",
     "shell.execute_reply": "2021-08-02T19:59:10.164042Z",
     "shell.execute_reply.started": "2021-08-02T17:27:04.091438Z"
    },
    "papermill": {
     "duration": 0.05767,
     "end_time": "2021-08-02T19:59:10.164687",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.107017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cognitive-bermuda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.241685Z",
     "iopub.status.busy": "2021-08-02T19:59:10.241043Z",
     "iopub.status.idle": "2021-08-02T19:59:10.447511Z",
     "shell.execute_reply": "2021-08-02T19:59:10.446402Z",
     "shell.execute_reply.started": "2021-08-02T17:27:04.118981Z"
    },
    "papermill": {
     "duration": 0.246254,
     "end_time": "2021-08-02T19:59:10.447662",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.201408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-garlic",
   "metadata": {
    "papermill": {
     "duration": 0.036424,
     "end_time": "2021-08-02T19:59:10.521522",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.485098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abstract-greensboro",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.603272Z",
     "iopub.status.busy": "2021-08-02T19:59:10.602357Z",
     "iopub.status.idle": "2021-08-02T19:59:10.604788Z",
     "shell.execute_reply": "2021-08-02T19:59:10.605238Z",
     "shell.execute_reply.started": "2021-08-02T17:27:04.328183Z"
    },
    "papermill": {
     "duration": 0.04754,
     "end_time": "2021-08-02T19:59:10.605378",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.557838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-saturday",
   "metadata": {
    "papermill": {
     "duration": 0.035815,
     "end_time": "2021-08-02T19:59:10.677587",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.641772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "growing-newspaper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.759012Z",
     "iopub.status.busy": "2021-08-02T19:59:10.758134Z",
     "iopub.status.idle": "2021-08-02T19:59:10.761191Z",
     "shell.execute_reply": "2021-08-02T19:59:10.760722Z",
     "shell.execute_reply.started": "2021-08-02T17:27:04.339802Z"
    },
    "papermill": {
     "duration": 0.047779,
     "end_time": "2021-08-02T19:59:10.761313",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.713534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eight-mustang",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.841027Z",
     "iopub.status.busy": "2021-08-02T19:59:10.840207Z",
     "iopub.status.idle": "2021-08-02T19:59:10.843188Z",
     "shell.execute_reply": "2021-08-02T19:59:10.842626Z",
     "shell.execute_reply.started": "2021-08-02T17:27:04.797897Z"
    },
    "papermill": {
     "duration": 0.045468,
     "end_time": "2021-08-02T19:59:10.843303",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.797835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-electricity",
   "metadata": {
    "papermill": {
     "duration": 0.035609,
     "end_time": "2021-08-02T19:59:10.914567",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.878958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifty-encounter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:10.997561Z",
     "iopub.status.busy": "2021-08-02T19:59:10.996925Z",
     "iopub.status.idle": "2021-08-02T19:59:11.014496Z",
     "shell.execute_reply": "2021-08-02T19:59:11.013915Z",
     "shell.execute_reply.started": "2021-08-02T17:27:05.764125Z"
    },
    "papermill": {
     "duration": 0.064146,
     "end_time": "2021-08-02T19:59:11.014627",
     "exception": false,
     "start_time": "2021-08-02T19:59:10.950481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "voluntary-chest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T19:59:11.101265Z",
     "iopub.status.busy": "2021-08-02T19:59:11.100341Z",
     "iopub.status.idle": "2021-08-02T20:00:07.382715Z",
     "shell.execute_reply": "2021-08-02T20:00:07.382146Z",
     "shell.execute_reply.started": "2021-08-02T17:27:06.260693Z"
    },
    "papermill": {
     "duration": 56.330441,
     "end_time": "2021-08-02T20:00:07.382865",
     "exception": false,
     "start_time": "2021-08-02T19:59:11.052424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in range(NUM_MODELS):            \n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "independent-poker",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:00:07.462936Z",
     "iopub.status.busy": "2021-08-02T20:00:07.462261Z",
     "iopub.status.idle": "2021-08-02T20:00:07.466576Z",
     "shell.execute_reply": "2021-08-02T20:00:07.466106Z",
     "shell.execute_reply.started": "2021-08-02T17:27:59.827728Z"
    },
    "papermill": {
     "duration": 0.046085,
     "end_time": "2021-08-02T20:00:07.466701",
     "exception": false,
     "start_time": "2021-08-02T20:00:07.420616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-green",
   "metadata": {
    "papermill": {
     "duration": 0.037119,
     "end_time": "2021-08-02T20:00:07.540979",
     "exception": false,
     "start_time": "2021-08-02T20:00:07.503860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2\n",
    "Imported from [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sexual-exclusive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:00:07.640620Z",
     "iopub.status.busy": "2021-08-02T20:00:07.630025Z",
     "iopub.status.idle": "2021-08-02T20:05:13.398124Z",
     "shell.execute_reply": "2021-08-02T20:05:13.398754Z",
     "shell.execute_reply.started": "2021-08-02T17:27:59.837091Z"
    },
    "papermill": {
     "duration": 305.8205,
     "end_time": "2021-08-02T20:05:13.398917",
     "exception": false,
     "start_time": "2021-08-02T20:00:07.578417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:12<04:50, 72.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:09<03:09, 63.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [03:07<02:01, 60.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [04:06<01:00, 60.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:00<00:00, 60.16s/it]\n"
     ]
    }
   ],
   "source": [
    "test = test_df\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids']\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                preds += logits\n",
    "        return preds\n",
    "\n",
    "def config(fold, model_name, load_model_path):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    \n",
    "    max_len = 256\n",
    "    batch_size = 8\n",
    "\n",
    "    model, tokenizer = make_model(\n",
    "        model_name=model_name, \n",
    "        num_labels=1\n",
    "    )\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )\n",
    "\n",
    "def run(fold=0, model_name=None, load_model_path=None):\n",
    "    model, tokenizer, \\\n",
    "        test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    \n",
    "    import time\n",
    "\n",
    "    evaluator = Evaluator(model, scaler)\n",
    "\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "\n",
    "    preds = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds\n",
    "\n",
    "pred_df1 = pd.DataFrame()\n",
    "pred_df2 = pd.DataFrame()\n",
    "pred_df3 = pd.DataFrame()\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df1[f'fold{fold}'] = run(fold, '../input/roberta-base/', '../input/commonlit-roberta-base-i/')\n",
    "    pred_df2[f'fold{fold+5}'] = run(fold, '../input/robertalarge/', '../input/roberta-large-itptfit/')\n",
    "    pred_df3[f'fold{fold+10}'] = run(fold, '../input/robertalarge/', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hourly-hardware",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:05:13.502694Z",
     "iopub.status.busy": "2021-08-02T20:05:13.502012Z",
     "iopub.status.idle": "2021-08-02T20:05:13.506107Z",
     "shell.execute_reply": "2021-08-02T20:05:13.505650Z",
     "shell.execute_reply.started": "2021-08-02T17:33:07.118242Z"
    },
    "papermill": {
     "duration": 0.059643,
     "end_time": "2021-08-02T20:05:13.506242",
     "exception": false,
     "start_time": "2021-08-02T20:05:13.446599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df1 = np.array(pred_df1)\n",
    "pred_df2 = np.array(pred_df2)\n",
    "pred_df3 = np.array(pred_df3)\n",
    "model2_predictions = (pred_df2.mean(axis=1)*0.65) + (pred_df1.mean(axis=1)*0.25) + (pred_df3.mean(axis=1) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "applied-sight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:05:13.604246Z",
     "iopub.status.busy": "2021-08-02T20:05:13.602650Z",
     "iopub.status.idle": "2021-08-02T20:05:13.605430Z",
     "shell.execute_reply": "2021-08-02T20:05:13.605921Z",
     "shell.execute_reply.started": "2021-08-02T17:33:07.12725Z"
    },
    "papermill": {
     "duration": 0.053588,
     "end_time": "2021-08-02T20:05:13.606065",
     "exception": false,
     "start_time": "2021-08-02T20:05:13.552477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_1 = model1_predictions * 0.5 + model2_predictions * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-window",
   "metadata": {
    "papermill": {
     "duration": 0.045319,
     "end_time": "2021-08-02T20:05:13.698251",
     "exception": false,
     "start_time": "2021-08-02T20:05:13.652932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Yum Yum fork**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "perfect-tower",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:05:13.794816Z",
     "iopub.status.busy": "2021-08-02T20:05:13.794203Z",
     "iopub.status.idle": "2021-08-02T20:05:13.798704Z",
     "shell.execute_reply": "2021-08-02T20:05:13.798238Z",
     "shell.execute_reply.started": "2021-08-02T17:40:10.429119Z"
    },
    "papermill": {
     "duration": 0.054569,
     "end_time": "2021-08-02T20:05:13.798824",
     "exception": false,
     "start_time": "2021-08-02T20:05:13.744255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-investigator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:05:13.898893Z",
     "iopub.status.busy": "2021-08-02T20:05:13.898093Z",
     "iopub.status.idle": "2021-08-02T20:05:13.900937Z",
     "shell.execute_reply": "2021-08-02T20:05:13.901478Z",
     "shell.execute_reply.started": "2021-08-02T17:40:11.275735Z"
    },
    "papermill": {
     "duration": 0.057064,
     "end_time": "2021-08-02T20:05:13.901629",
     "exception": false,
     "start_time": "2021-08-02T20:05:13.844565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, excerpt, tokenizer, max_len):\n",
    "        self.excerpt = excerpt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.excerpt[item])\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_len, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ruled-trademark",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:05:14.005659Z",
     "iopub.status.busy": "2021-08-02T20:05:14.003921Z",
     "iopub.status.idle": "2021-08-02T20:05:14.006377Z",
     "shell.execute_reply": "2021-08-02T20:05:14.006837Z",
     "shell.execute_reply.started": "2021-08-02T17:40:12.686437Z"
    },
    "papermill": {
     "duration": 0.058764,
     "end_time": "2021-08-02T20:05:14.006980",
     "exception": false,
     "start_time": "2021-08-02T20:05:13.948216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model_path, max_len):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "    \n",
    "    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=False\n",
    "    )\n",
    "\n",
    "    final_output = []\n",
    "\n",
    "    for b_idx, data in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(\"cuda\")\n",
    "            output = model(**data)\n",
    "            output = output.logits.detach().cpu().numpy().ravel().tolist()\n",
    "            final_output.extend(output)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return np.array(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unlike-funeral",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:05:14.105811Z",
     "iopub.status.busy": "2021-08-02T20:05:14.104996Z",
     "iopub.status.idle": "2021-08-02T20:07:03.177402Z",
     "shell.execute_reply": "2021-08-02T20:07:03.177962Z",
     "shell.execute_reply.started": "2021-08-02T17:40:13.53046Z"
    },
    "papermill": {
     "duration": 109.124519,
     "end_time": "2021-08-02T20:07:03.178159",
     "exception": false,
     "start_time": "2021-08-02T20:05:14.053640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds1 = generate_predictions(\"../input/a81653/\", max_len=512)\n",
    "preds2 = generate_predictions(\"../input/a81656/\", max_len=512)\n",
    "preds3 = generate_predictions(\"../input/a81657/\", max_len=512)\n",
    "preds4 = generate_predictions(\"../input/a81660/\", max_len=512)\n",
    "preds5 = generate_predictions(\"../input/a81675/\", max_len=192)\n",
    "preds6 = generate_predictions(\"../input/a87832/\", max_len=512)\n",
    "\n",
    "preds_yumyum = (preds1 + preds2 + preds3 + preds4 + preds5 + preds6) / 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-jaguar",
   "metadata": {
    "papermill": {
     "duration": 0.044581,
     "end_time": "2021-08-02T20:07:03.269117",
     "exception": false,
     "start_time": "2021-08-02T20:07:03.224536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**3X inference notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ambient-samba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:03.365968Z",
     "iopub.status.busy": "2021-08-02T20:07:03.365193Z",
     "iopub.status.idle": "2021-08-02T20:07:03.369131Z",
     "shell.execute_reply": "2021-08-02T20:07:03.368653Z",
     "shell.execute_reply.started": "2021-08-02T19:53:23.886518Z"
    },
    "papermill": {
     "duration": 0.055824,
     "end_time": "2021-08-02T20:07:03.369254",
     "exception": false,
     "start_time": "2021-08-02T20:07:03.313430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vocational-messenger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:03.468438Z",
     "iopub.status.busy": "2021-08-02T20:07:03.467439Z",
     "iopub.status.idle": "2021-08-02T20:07:03.472267Z",
     "shell.execute_reply": "2021-08-02T20:07:03.471785Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.497373Z"
    },
    "papermill": {
     "duration": 0.05871,
     "end_time": "2021-08-02T20:07:03.472393",
     "exception": false,
     "start_time": "2021-08-02T20:07:03.413683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 248\n",
    "EVAL_SCHEDULE = [(0.5, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1, 1)]\n",
    "ROBERTA_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\n",
    "TOKENIZER_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dense-patent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:03.574694Z",
     "iopub.status.busy": "2021-08-02T20:07:03.574062Z",
     "iopub.status.idle": "2021-08-02T20:07:03.581218Z",
     "shell.execute_reply": "2021-08-02T20:07:03.580584Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.567841Z"
    },
    "papermill": {
     "duration": 0.062607,
     "end_time": "2021-08-02T20:07:03.581347",
     "exception": false,
     "start_time": "2021-08-02T20:07:03.518740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nearby-rochester",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:03.677998Z",
     "iopub.status.busy": "2021-08-02T20:07:03.677352Z",
     "iopub.status.idle": "2021-08-02T20:07:03.914438Z",
     "shell.execute_reply": "2021-08-02T20:07:03.913501Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.593177Z"
    },
    "papermill": {
     "duration": 0.286953,
     "end_time": "2021-08-02T20:07:03.914601",
     "exception": false,
     "start_time": "2021-08-02T20:07:03.627648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "transparent-growth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:04.017614Z",
     "iopub.status.busy": "2021-08-02T20:07:04.016881Z",
     "iopub.status.idle": "2021-08-02T20:07:04.020390Z",
     "shell.execute_reply": "2021-08-02T20:07:04.019918Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.802815Z"
    },
    "papermill": {
     "duration": 0.059118,
     "end_time": "2021-08-02T20:07:04.020537",
     "exception": false,
     "start_time": "2021-08-02T20:07:03.961419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "respected-techno",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:04.122959Z",
     "iopub.status.busy": "2021-08-02T20:07:04.122150Z",
     "iopub.status.idle": "2021-08-02T20:07:04.126594Z",
     "shell.execute_reply": "2021-08-02T20:07:04.125805Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.815881Z"
    },
    "papermill": {
     "duration": 0.059817,
     "end_time": "2021-08-02T20:07:04.126796",
     "exception": false,
     "start_time": "2021-08-02T20:07:04.066979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.25,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "written-width",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:04.227651Z",
     "iopub.status.busy": "2021-08-02T20:07:04.226879Z",
     "iopub.status.idle": "2021-08-02T20:07:04.230273Z",
     "shell.execute_reply": "2021-08-02T20:07:04.229808Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.829910Z"
    },
    "papermill": {
     "duration": 0.056547,
     "end_time": "2021-08-02T20:07:04.230409",
     "exception": false,
     "start_time": "2021-08-02T20:07:04.173862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "civil-respect",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:04.336106Z",
     "iopub.status.busy": "2021-08-02T20:07:04.335163Z",
     "iopub.status.idle": "2021-08-02T20:07:52.411710Z",
     "shell.execute_reply": "2021-08-02T20:07:52.410817Z",
     "shell.execute_reply.started": "2021-08-02T19:53:26.841454Z"
    },
    "papermill": {
     "duration": 48.135104,
     "end_time": "2021-08-02T20:07:52.411862",
     "exception": false,
     "start_time": "2021-08-02T20:07:04.276758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:13<00:54, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:21<00:31, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:30<00:19,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:39<00:09,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:48<00:00,  9.61s/it]\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in tqdm(range(NUM_MODELS)):            \n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "velvet-silver",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:52.516329Z",
     "iopub.status.busy": "2021-08-02T20:07:52.515743Z",
     "iopub.status.idle": "2021-08-02T20:07:52.519614Z",
     "shell.execute_reply": "2021-08-02T20:07:52.520196Z",
     "shell.execute_reply.started": "2021-08-02T19:54:21.134573Z"
    },
    "papermill": {
     "duration": 0.058427,
     "end_time": "2021-08-02T20:07:52.520362",
     "exception": false,
     "start_time": "2021-08-02T20:07:52.461935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "advisory-direction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:52.628960Z",
     "iopub.status.busy": "2021-08-02T20:07:52.627364Z",
     "iopub.status.idle": "2021-08-02T20:07:52.629978Z",
     "shell.execute_reply": "2021-08-02T20:07:52.630464Z",
     "shell.execute_reply.started": "2021-08-02T19:54:21.141302Z"
    },
    "papermill": {
     "duration": 0.060656,
     "end_time": "2021-08-02T20:07:52.630613",
     "exception": false,
     "start_time": "2021-08-02T20:07:52.569957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_df\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "determined-michigan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:52.745412Z",
     "iopub.status.busy": "2021-08-02T20:07:52.744639Z",
     "iopub.status.idle": "2021-08-02T20:07:52.748297Z",
     "shell.execute_reply": "2021-08-02T20:07:52.747795Z",
     "shell.execute_reply.started": "2021-08-02T19:54:25.729833Z"
    },
    "papermill": {
     "duration": 0.067918,
     "end_time": "2021-08-02T20:07:52.748432",
     "exception": false,
     "start_time": "2021-08-02T20:07:52.680514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "applicable-interest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:52.867021Z",
     "iopub.status.busy": "2021-08-02T20:07:52.866209Z",
     "iopub.status.idle": "2021-08-02T20:07:52.870141Z",
     "shell.execute_reply": "2021-08-02T20:07:52.869680Z",
     "shell.execute_reply.started": "2021-08-02T19:54:25.743690Z"
    },
    "papermill": {
     "duration": 0.069864,
     "end_time": "2021-08-02T20:07:52.870268",
     "exception": false,
     "start_time": "2021-08-02T20:07:52.800404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        \n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "            \n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "legislative-timer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:52.977897Z",
     "iopub.status.busy": "2021-08-02T20:07:52.977102Z",
     "iopub.status.idle": "2021-08-02T20:07:52.981112Z",
     "shell.execute_reply": "2021-08-02T20:07:52.980643Z",
     "shell.execute_reply.started": "2021-08-02T19:54:25.760582Z"
    },
    "papermill": {
     "duration": 0.061043,
     "end_time": "2021-08-02T20:07:52.981236",
     "exception": false,
     "start_time": "2021-08-02T20:07:52.920193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "minute-relative",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:53.098412Z",
     "iopub.status.busy": "2021-08-02T20:07:53.096567Z",
     "iopub.status.idle": "2021-08-02T20:07:53.099212Z",
     "shell.execute_reply": "2021-08-02T20:07:53.099703Z",
     "shell.execute_reply.started": "2021-08-02T19:54:25.773428Z"
    },
    "papermill": {
     "duration": 0.068507,
     "end_time": "2021-08-02T20:07:53.099855",
     "exception": false,
     "start_time": "2021-08-02T20:07:53.031348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids']\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                preds += logits\n",
    "        return preds\n",
    "\n",
    "def config(fold, model_name, load_model_path):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    \n",
    "    max_len = 250\n",
    "    batch_size = 8\n",
    "\n",
    "    model, tokenizer = make_model(\n",
    "        model_name=model_name, \n",
    "        num_labels=1\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    \n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "critical-audit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:53.207137Z",
     "iopub.status.busy": "2021-08-02T20:07:53.206334Z",
     "iopub.status.idle": "2021-08-02T20:07:53.210225Z",
     "shell.execute_reply": "2021-08-02T20:07:53.209725Z",
     "shell.execute_reply.started": "2021-08-02T19:54:25.788596Z"
    },
    "papermill": {
     "duration": 0.060215,
     "end_time": "2021-08-02T20:07:53.210347",
     "exception": false,
     "start_time": "2021-08-02T20:07:53.150132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(fold=0, model_name=None, load_model_path=None):\n",
    "    model, tokenizer, \\\n",
    "        test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    \n",
    "    import time\n",
    "\n",
    "    evaluator = Evaluator(model, scaler)\n",
    "\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "\n",
    "    preds = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "alternative-cinema",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:07:53.317187Z",
     "iopub.status.busy": "2021-08-02T20:07:53.316580Z",
     "iopub.status.idle": "2021-08-02T20:12:39.052856Z",
     "shell.execute_reply": "2021-08-02T20:12:39.052300Z"
    },
    "papermill": {
     "duration": 285.79309,
     "end_time": "2021-08-02T20:12:39.053011",
     "exception": false,
     "start_time": "2021-08-02T20:07:53.259921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:15<05:02, 75.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:07<03:05, 61.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [02:59<01:54, 57.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [03:52<00:55, 55.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:45<00:00, 57.15s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_df1 = pd.DataFrame()\n",
    "pred_df2 = pd.DataFrame()\n",
    "pred_df3 = pd.DataFrame()\n",
    "\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df1[f'fold{fold}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-base/', '../input/commonlit-roberta-base-i/')\n",
    "    pred_df2[f'fold{fold+5}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/roberta-large-itptfit/')\n",
    "    pred_df3[f'fold{fold+10}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "automated-samuel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:12:39.178622Z",
     "iopub.status.busy": "2021-08-02T20:12:39.177961Z",
     "iopub.status.idle": "2021-08-02T20:12:39.181685Z",
     "shell.execute_reply": "2021-08-02T20:12:39.182098Z"
    },
    "papermill": {
     "duration": 0.071866,
     "end_time": "2021-08-02T20:12:39.182264",
     "exception": false,
     "start_time": "2021-08-02T20:12:39.110398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df1 = np.array(pred_df1)\n",
    "pred_df2 = np.array(pred_df2)\n",
    "pred_df3 = np.array(pred_df3)\n",
    "\n",
    "model2_predictions = (pred_df2.mean(axis=1) * 0.5) + (pred_df1.mean(axis=1) * 0.3) + (pred_df3.mean(axis=1) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "accepting-mayor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:12:39.304171Z",
     "iopub.status.busy": "2021-08-02T20:12:39.303355Z",
     "iopub.status.idle": "2021-08-02T20:12:39.489642Z",
     "shell.execute_reply": "2021-08-02T20:12:39.489078Z"
    },
    "papermill": {
     "duration": 0.250451,
     "end_time": "2021-08-02T20:12:39.489780",
     "exception": false,
     "start_time": "2021-08-02T20:12:39.239329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gc; gc.enable()\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "saved-garden",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:12:39.612943Z",
     "iopub.status.busy": "2021-08-02T20:12:39.612317Z",
     "iopub.status.idle": "2021-08-02T20:12:39.616480Z",
     "shell.execute_reply": "2021-08-02T20:12:39.615981Z"
    },
    "papermill": {
     "duration": 0.069829,
     "end_time": "2021-08-02T20:12:39.616609",
     "exception": false,
     "start_time": "2021-08-02T20:12:39.546780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/commonlitreadabilityprize'\n",
    "MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n",
    "CHECKPOINT_DIR = '../input/clrp-mean-pooling/'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAX_LENGTH = 248\n",
    "TEST_BATCH_SIZE = 1\n",
    "HIDDEN_SIZE = 1024\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "SEEDS = [113]\n",
    "\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "under-occurrence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:12:39.741583Z",
     "iopub.status.busy": "2021-08-02T20:12:39.739755Z",
     "iopub.status.idle": "2021-08-02T20:12:39.742346Z",
     "shell.execute_reply": "2021-08-02T20:12:39.742820Z"
    },
    "papermill": {
     "duration": 0.068852,
     "end_time": "2021-08-02T20:12:39.742968",
     "exception": false,
     "start_time": "2021-08-02T20:12:39.674116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPoolingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        logits = self.linear(mean_embeddings)\n",
    "        \n",
    "        preds = logits.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n",
    "            return loss\n",
    "        else:\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "filled-runner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:12:39.863395Z",
     "iopub.status.busy": "2021-08-02T20:12:39.862624Z",
     "iopub.status.idle": "2021-08-02T20:12:40.051627Z",
     "shell.execute_reply": "2021-08-02T20:12:40.051032Z"
    },
    "papermill": {
     "duration": 0.252184,
     "end_time": "2021-08-02T20:12:40.051772",
     "exception": false,
     "start_time": "2021-08-02T20:12:39.799588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_loader(data):\n",
    "\n",
    "    x_test = data.excerpt.tolist()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    encoded_test = tokenizer.batch_encode_plus(\n",
    "        x_test, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    dataset_test = TensorDataset(\n",
    "        encoded_test['input_ids'],\n",
    "        encoded_test['attention_mask']\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        sampler = SequentialSampler(dataset_test),\n",
    "        batch_size=TEST_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return dataloader_test\n",
    "\n",
    "test_dataloader = get_test_loader(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "serial-albany",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:12:40.204264Z",
     "iopub.status.busy": "2021-08-02T20:12:40.203201Z",
     "iopub.status.idle": "2021-08-02T20:14:35.080753Z",
     "shell.execute_reply": "2021-08-02T20:14:35.081176Z"
    },
    "papermill": {
     "duration": 114.972173,
     "end_time": "2021-08-02T20:14:35.081350",
     "exception": false,
     "start_time": "2021-08-02T20:12:40.109177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd0c63880ce40dda26aa9accce2631c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_114_1.pth\n",
      "\n",
      "Using model_114_2.pth\n",
      "\n",
      "Using model_114_3.pth\n",
      "\n",
      "Using model_114_4.pth\n",
      "\n",
      "Using model_114_5.pth\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "for seed in SEEDS:\n",
    "    \n",
    "    fold_predictions = []\n",
    "    \n",
    "    for fold in tqdm(range(NUM_FOLDS)):\n",
    "        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "        \n",
    "        print(f\"\\nUsing {model_path}\")\n",
    "        \n",
    "        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "        model = MeanPoolingModel(MODEL_DIR)\n",
    "        model.load_state_dict(torch.load(model_path)) \n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         None,\n",
    "                     }\n",
    "\n",
    "     \n",
    "            preds = model(**inputs).item()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "        del model \n",
    "        gc.collect()\n",
    "            \n",
    "        fold_predictions.append(predictions)\n",
    "    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n",
    "    \n",
    "model3_predictions = np.mean(all_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tracked-fiber",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:14:35.216285Z",
     "iopub.status.busy": "2021-08-02T20:14:35.215429Z",
     "iopub.status.idle": "2021-08-02T20:14:35.219245Z",
     "shell.execute_reply": "2021-08-02T20:14:35.219760Z"
    },
    "papermill": {
     "duration": 0.080129,
     "end_time": "2021-08-02T20:14:35.219916",
     "exception": false,
     "start_time": "2021-08-02T20:14:35.139787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42657179, -0.57216584, -0.40823111, -2.43045233, -1.76784014,\n",
       "       -1.27811189,  0.09320229])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_3x = model1_predictions * 0.5 + model2_predictions * 0.3 + model3_predictions * 0.2  \n",
    "predictions_3x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-visit",
   "metadata": {
    "papermill": {
     "duration": 0.059251,
     "end_time": "2021-08-02T20:14:35.339869",
     "exception": false,
     "start_time": "2021-08-02T20:14:35.280618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ensemble predictions from top models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abstract-metallic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T20:14:35.483204Z",
     "iopub.status.busy": "2021-08-02T20:14:35.482441Z",
     "iopub.status.idle": "2021-08-02T20:14:36.331149Z",
     "shell.execute_reply": "2021-08-02T20:14:36.330550Z"
    },
    "papermill": {
     "duration": 0.932784,
     "end_time": "2021-08-02T20:14:36.331280",
     "exception": false,
     "start_time": "2021-08-02T20:14:35.398496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id    target\n",
      "0  c0f722661 -0.411553\n",
      "1  f0953f0a5 -0.568449\n",
      "2  0df072751 -0.404413\n",
      "3  04caf4e0c -2.428176\n",
      "4  0e63f8bea -1.770321\n",
      "5  12537fe78 -1.276877\n",
      "6  965e592c0  0.106785\n"
     ]
    }
   ],
   "source": [
    "submission_df.target = np.array(predictions_3x)*0.7+np.array(predictions_1)*0.25+np.array(preds_yumyum)*.05\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-organic",
   "metadata": {
    "papermill": {
     "duration": 0.060278,
     "end_time": "2021-08-02T20:14:36.465575",
     "exception": false,
     "start_time": "2021-08-02T20:14:36.405297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 941.090759,
   "end_time": "2021-08-02T20:14:40.279312",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-02T19:58:59.188553",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02527b2a33c34bc0b81ef4b0e5493e21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "283c999e6bcb47308dfa460614a34cd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4e435c3550d6459796d89b4b125803fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "965c485a9a794fc69f699cc6a312584d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c882c97d67c245bf9b558f145d038077",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a7f6643f1da64d95ae84ca9c900a4d8a",
       "value": 5.0
      }
     },
     "9bd0c63880ce40dda26aa9accce2631c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a853bda213ca4672b9903b3d9663663c",
        "IPY_MODEL_965c485a9a794fc69f699cc6a312584d",
        "IPY_MODEL_d37d2facea3b4e40a217675ebd5d7c78"
       ],
       "layout": "IPY_MODEL_c033c556c5174abcade7952630a9ab9d"
      }
     },
     "a7f6643f1da64d95ae84ca9c900a4d8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a853bda213ca4672b9903b3d9663663c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_02527b2a33c34bc0b81ef4b0e5493e21",
       "placeholder": "​",
       "style": "IPY_MODEL_4e435c3550d6459796d89b4b125803fe",
       "value": "100%"
      }
     },
     "c033c556c5174abcade7952630a9ab9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c882c97d67c245bf9b558f145d038077": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d37d2facea3b4e40a217675ebd5d7c78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efa3156fae844f5da1709ffdb28273c5",
       "placeholder": "​",
       "style": "IPY_MODEL_283c999e6bcb47308dfa460614a34cd8",
       "value": " 5/5 [01:54&lt;00:00, 22.95s/it]"
      }
     },
     "efa3156fae844f5da1709ffdb28273c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
