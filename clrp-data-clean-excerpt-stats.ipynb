{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebc1246",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-06T19:15:20.519151Z",
     "iopub.status.busy": "2021-07-06T19:15:20.518775Z",
     "iopub.status.idle": "2021-07-06T19:15:20.530932Z",
     "shell.execute_reply": "2021-07-06T19:15:20.52949Z",
     "shell.execute_reply.started": "2021-07-06T19:15:20.519126Z"
    },
    "papermill": {
     "duration": 0.012622,
     "end_time": "2021-07-09T15:35:40.347831",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.335209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Cleaning & Data Stats\n",
    "\n",
    "The goal of the notebook is to clean CommonLit Data and get some statistics of the excerpt text. The code uses NLTK, regex, and pandas manipulation to achieve this.\n",
    "\n",
    "Here are the various processes in this notebook:\n",
    "\n",
    "1. Reading Data\n",
    "2. Clean the Data\n",
    "3. Get important stats about the excerpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a9cef",
   "metadata": {
    "papermill": {
     "duration": 0.011278,
     "end_time": "2021-07-09T15:35:40.371343",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.360065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Process 1: Reading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aca860d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:40.408057Z",
     "iopub.status.busy": "2021-07-09T15:35:40.407029Z",
     "iopub.status.idle": "2021-07-09T15:35:40.413036Z",
     "shell.execute_reply": "2021-07-09T15:35:40.412092Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.210275Z"
    },
    "papermill": {
     "duration": 0.030134,
     "end_time": "2021-07-09T15:35:40.413217",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.383083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n",
      "/kaggle/input/commonlitreadabilityprize/train.csv\n",
      "/kaggle/input/commonlitreadabilityprize/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19e008",
   "metadata": {
    "papermill": {
     "duration": 0.012331,
     "end_time": "2021-07-09T15:35:40.438195",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.425864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Verify that the data import and explore the data by printing the top 3 rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d843824f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:40.467970Z",
     "iopub.status.busy": "2021-07-09T15:35:40.467193Z",
     "iopub.status.idle": "2021-07-09T15:35:40.684004Z",
     "shell.execute_reply": "2021-07-09T15:35:40.684536Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.223083Z"
    },
    "papermill": {
     "duration": 0.23432,
     "end_time": "2021-07-09T15:35:40.684712",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.450392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv(os.path.join(dirname, filenames[1]))\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01690d8b",
   "metadata": {
    "papermill": {
     "duration": 0.011967,
     "end_time": "2021-07-09T15:35:40.708959",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.696992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Process 2: Clean the Data**\n",
    "\n",
    "Convert the excerpt into lower case(so that an accurate count of the words can be obtained).\n",
    "Then remove the most frequently occurring words like - 'an', 'the', and 'on'. This list of frequently occurring can be obtained from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1ee61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:40.736510Z",
     "iopub.status.busy": "2021-07-09T15:35:40.735863Z",
     "iopub.status.idle": "2021-07-09T15:35:42.607763Z",
     "shell.execute_reply": "2021-07-09T15:35:42.608660Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.287396Z"
    },
    "papermill": {
     "duration": 1.887814,
     "end_time": "2021-07-09T15:35:42.608904",
     "exception": false,
     "start_time": "2021-07-09T15:35:40.721090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "train_df['excerpt_preprocess']=train_df['excerpt'].str.lower()\n",
    "train_df['excerpt_preprocess']=train_df['excerpt_preprocess'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
    "train_df['excerpt_length']=train_df['excerpt_preprocess'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e746d8c",
   "metadata": {
    "papermill": {
     "duration": 0.011933,
     "end_time": "2021-07-09T15:35:42.634087",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.622154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****Process 3: Get excerpt stats****\n",
    "\n",
    "Get the initial length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e572fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:42.662940Z",
     "iopub.status.busy": "2021-07-09T15:35:42.662031Z",
     "iopub.status.idle": "2021-07-09T15:35:42.670227Z",
     "shell.execute_reply": "2021-07-09T15:35:42.670925Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.425556Z"
    },
    "papermill": {
     "duration": 0.024533,
     "end_time": "2021-07-09T15:35:42.671127",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.646594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       young people returned ballroom, presented deci...\n",
       "1       dinner time, mrs. fayre somewhat silent, eyes ...\n",
       "2       roger predicted, snow departed quickly came, t...\n",
       "3       outside palace great garden walled round, fill...\n",
       "4       upon time three bears lived together house woo...\n",
       "                              ...                        \n",
       "2829    think dinosaurs lived, picture? see hot, steam...\n",
       "2830    solid? solids usually hard molecules packed to...\n",
       "2831    second state matter discuss liquid. solids har...\n",
       "2832    solids shapes actually touch. three dimensions...\n",
       "2833    animals made many cells. eat things digest ins...\n",
       "Name: excerpt_preprocess, Length: 2834, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['excerpt_preprocess']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cc3c5",
   "metadata": {
    "papermill": {
     "duration": 0.012363,
     "end_time": "2021-07-09T15:35:42.696360",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.683997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Get the average word count of each excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd27b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:42.726204Z",
     "iopub.status.busy": "2021-07-09T15:35:42.725502Z",
     "iopub.status.idle": "2021-07-09T15:35:42.731506Z",
     "shell.execute_reply": "2021-07-09T15:35:42.732038Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.43461Z"
    },
    "papermill": {
     "duration": 0.02241,
     "end_time": "2021-07-09T15:35:42.732219",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.709809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(train_df['excerpt_length'].mean(),0) #Avg. excerpt length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5ef57",
   "metadata": {
    "papermill": {
     "duration": 0.01268,
     "end_time": "2021-07-09T15:35:42.757955",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.745275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Extract the first text excerpt and analyze the average length of the sentence(words in a sentence), the maximum length of the sentence, and the maximum punctuations in a sentence.\n",
    "\n",
    "The idea is to correlate word count, sentence length, and punctuations to the text complexity. This is done initially for the raw text and then for the text where it was preprocessed(lowercased and with the frequent text removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92492080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:42.788708Z",
     "iopub.status.busy": "2021-07-09T15:35:42.787913Z",
     "iopub.status.idle": "2021-07-09T15:35:42.817432Z",
     "shell.execute_reply": "2021-07-09T15:35:42.816859Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.447482Z"
    },
    "papermill": {
     "duration": 0.046333,
     "end_time": "2021-07-09T15:35:42.817638",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.771305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sent Length:  89.3\n",
      "Max sent Length:  142\n",
      "Max punct Length:  4\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import re\n",
    "\n",
    "excerpt_1 = train_df.loc[0,'excerpt']\n",
    "sent_1 = sent_tokenize(excerpt_1)\n",
    "\n",
    "max_len=0\n",
    "count=0\n",
    "total_length=0\n",
    "punct=\";|!|:|;|,|-|'\"\n",
    "max_punct_len=0\n",
    "\n",
    "for sent in sent_1:\n",
    "    punct_len=len(re.findall(punct, sent))\n",
    "    if punct_len>max_punct_len:\n",
    "        max_punct_len=punct_len\n",
    "    if len(sent)>max_len:\n",
    "        max_len=len(sent)\n",
    "    total_length+=len(sent)\n",
    "    count+=1\n",
    "print(\"Average sent Length: \",round(total_length/count,1))\n",
    "print(\"Max sent Length: \",max_len)\n",
    "print(\"Max punct Length: \",max_punct_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee24798",
   "metadata": {
    "papermill": {
     "duration": 0.01376,
     "end_time": "2021-07-09T15:35:42.846241",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.832481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Below shows the same stats, but now with the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf46f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T15:35:42.883260Z",
     "iopub.status.busy": "2021-07-09T15:35:42.882431Z",
     "iopub.status.idle": "2021-07-09T15:35:42.887318Z",
     "shell.execute_reply": "2021-07-09T15:35:42.887855Z",
     "shell.execute_reply.started": "2021-07-09T15:33:06.476988Z"
    },
    "papermill": {
     "duration": 0.028197,
     "end_time": "2021-07-09T15:35:42.888041",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.859844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sent Length:  57.5\n",
      "Max sent Length:  93\n",
      "Max punct Length:  4\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import re\n",
    "\n",
    "excerpt_1 = train_df.loc[0,'excerpt_preprocess']\n",
    "sent_1 = sent_tokenize(excerpt_1)\n",
    "\n",
    "max_len=0\n",
    "count=0\n",
    "total_length=0\n",
    "punct=\";|!|:|;|,|-|'\"\n",
    "max_punct_len=0\n",
    "\n",
    "for sent in sent_1:\n",
    "    punct_len=len(re.findall(punct, sent))\n",
    "    if punct_len>max_punct_len:\n",
    "        max_punct_len=punct_len\n",
    "    if len(sent)>max_len:\n",
    "        max_len=len(sent)\n",
    "    total_length+=len(sent)\n",
    "    count+=1\n",
    "print(\"Average sent Length: \",round(total_length/count,1))\n",
    "print(\"Max sent Length: \",max_len)\n",
    "print(\"Max punct Length: \",max_punct_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe26a92",
   "metadata": {
    "papermill": {
     "duration": 0.013474,
     "end_time": "2021-07-09T15:35:42.915318",
     "exception": false,
     "start_time": "2021-07-09T15:35:42.901844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The next steps are to get the counts for all the excerpts and then encode the text. This data will then be fed to an ML model to rank/access the excerpt complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.638043,
   "end_time": "2021-07-09T15:35:43.740550",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-09T15:35:31.102507",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
